[//]: # (<br />)
<p align="center">
  <h1 align="center">中国科学院大学本科部 2025 NLP 作业</h1>
  <p align="center">
    <img src="https://img.shields.io/badge/NLP-Homework-blue?style=flat&logo=github" alt="NLP Status">
    <img src="https://img.shields.io/badge/Python-NLP%20%7C%20Crawler-green" alt="Python">
  </p>
</p>

## Homework1（对应文件夹hw1）
### 题目
1. (简答题)

请从下列三个题目中任选一个作答：

(A). 利用网络爬虫工具从互联网上收集尽量多的英语和汉语文本，完成下面的工作： 

① 对收集的样本进行处理，如清洗乱码等。

② 设计并编程实现算法，计算在收集样本上英语字母和单词或汉字的概率和熵（如果有汉语自动分词工具，可以统计计算汉语词汇的概率和熵）。

③ 利用收集的英文文本验证齐夫定律(Zipf's law)。

④ 不断增加样本规模，重新计算上面②③的结果, 进行对比分析。

⑤ 完成一份技术报告，在报告中写明利用什么爬虫工具从哪些网站上收集的样本，如何进行的样本清洗，清洗后样本的规模，在不同样本规模下计算的结果等。实验分析有较大的伸缩空间。



(B). 利用网络爬虫工具从互联网上收集尽量多的汉语文本，完成下面的工作：

① 对收集的样本进行处理，如清洗乱码等。

② 设计并编程实现算法，计算任意两个汉字邻近同现的概率(如果有汉语自动分词工具，可以统计计算汉语词汇的邻近同现概率)。 

③ 计算任意两个汉字之间的(点式)互信息。 

④ 计算任意两篇文章之间的(平均)互信息。 

⑤ 更改样本来源或类型，如来自不同网站的语料或者不同类型的语料等，重新计算上面②③④的结果, 进行对比分析。 

⑥ 完成一份技术报告，在报告中写明利用什么爬虫工具从哪些网站上收集的样本，如何进行的样本清洗，清洗后样本的规模，计算结果分析等。实验分析有较大的伸缩空间。



(C). 利用北京大学标注的《人民日报》1998年1月份的分词和词性标注语料（见SEP平台本课程附录03）完成如下工作： 

① 设计并实现算法，计算任意两个汉字之间的点式互信息和它们邻近同现的概率，分析两个邻近汉字构成词汇的概率与它们之间点式互信息情况。 

② 根据词性分类标记，计算任意两类词汇之间的互信息。 

③ 利用网络爬虫从《人民日报》官网(https ://paper.people.com.cn/rmrb/pc/layout/202510/15/node_01.html) 上爬取尽量多的语料（包括今年1月份的内容），对其进行处理，如清洗乱码等。 

④ 利用③中获取的今年1月份的语料与1998年1月的《人民日报》语料进行对比分析，分析词频差异，并抽取出所有的新词。 

⑤ 完成一份技术报告，在报告中写明上述各任务的计算结果分析情况，利用什么网络爬虫工具，如何进行的样本清洗等。结果分析有较大的伸缩空间。

**鉴于笔者在题目要求修改之前已经完成了作业，所以笔者选择了和原来相似度较大的问题一进行作业。**

### 具体内容见hw1中的README.md

### 总结

从上述数据可以看到，中英文语料的熵值和高频符号分布都相对稳定：中文语料在 2MB/5MB/10MB 三个快照中的熵值维持在 9.5~9.64 bits 之间，高频字始终集中在“的、国、中、一”等功能词或常用名词；英文语料的字母熵值则稳定在 4.16 bits 左右，高频字母分布符合英语普通文本 `ETAOIN` 的典型规律。在英文单词统计中，将所有撇号形式的所有格结尾（如 `China's`、`people’s`）视为整词处理后，孤立的 `s` 不再计入词频，因此 top-10 高频词更贴近常见功能词分布，整体熵值也随语料规模上升略有增加（约 10.29→10.43 bits），表明词级别的多样性与语料扩展保持正相关。总体而言随着语料规模扩大，字符覆盖度和去重数量逐渐提升，但整体熵值变化不大，说明单字符层面的统计特征在较小规模就已趋于收敛。后续分析可进一步考察不同时间段、题材或版面的差异，并尝试扩展到二元或 n 元模型，以捕捉更丰富的语言结构信息。


## Homework2（对应文件夹hw2）

### 题目

1. (简答题)

  请下载调试FNN、RNN和LSTM模型的开源工具。利用北京大学标注的《人民日报》1998年1月份的分词语料，或者利用网络爬虫自己从互联网上收集足够多的英文文本语料，借助FNN或者RNN/LSTM开源工具，完成如下任务，并撰写一份实验报告：

  (1) 获得汉语或英语词语的词向量. 

  (2) 对于同一批词汇，对比分别用FNN,RNN或LSTM获得的词向量的差异。 

  (3) 利用你认为最好的词向量结果，对于随机选取的20个词汇分别计算与其词向量最相似的前10个单词，按相似度大小排序，人工对比排序结果是否与你的判断一致。

  (4) [选做]如果汉语和英语的词向量都学习到了，请对比同一个意思的汉语词汇和英语词汇，如“书”和‘book’，“工作”和‘work/ job’ 等，分析其向量距离。



说明

- 如果计算资源的限制，神经网络参数不必选择过大，例如：词表选择1000个左右单词即可, 其余单词用代替；词向量的维度可设为10左右；神经网络的层数设置为1到2层； 

- 可以使用某一种开放的深度学习框架，如TensorFlow或者PyTorch。

- 如果不借助开源工具和开放的深度学习框架，题目中的某些任务可以不做。



