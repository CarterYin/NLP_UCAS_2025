[//]: # (<br />)
<p align="center">
  <h1 align="center">中国科学院大学本科部 2025 NLP 作业</h1>
  <p align="center">
    <img src="https://img.shields.io/badge/NLP-Homework-blue?style=flat&logo=github" alt="NLP Status">
    <img src="https://img.shields.io/badge/Python-NLP%20%7C%20Crawler-green" alt="Python">
  </p>
</p>

## Homework1（对应文件夹hw1）
### 题目
1. (简答题)

请从下列三个题目中任选一个作答：

(A). 利用网络爬虫工具从互联网上收集尽量多的英语和汉语文本，完成下面的工作： 

① 对收集的样本进行处理，如清洗乱码等。

② 设计并编程实现算法，计算在收集样本上英语字母和单词或汉字的概率和熵（如果有汉语自动分词工具，可以统计计算汉语词汇的概率和熵）。

③ 利用收集的英文文本验证齐夫定律(Zipf's law)。

④ 不断增加样本规模，重新计算上面②③的结果, 进行对比分析。

⑤ 完成一份技术报告，在报告中写明利用什么爬虫工具从哪些网站上收集的样本，如何进行的样本清洗，清洗后样本的规模，在不同样本规模下计算的结果等。实验分析有较大的伸缩空间。



(B). 利用网络爬虫工具从互联网上收集尽量多的汉语文本，完成下面的工作：

① 对收集的样本进行处理，如清洗乱码等。

② 设计并编程实现算法，计算任意两个汉字邻近同现的概率(如果有汉语自动分词工具，可以统计计算汉语词汇的邻近同现概率)。 

③ 计算任意两个汉字之间的(点式)互信息。 

④ 计算任意两篇文章之间的(平均)互信息。 

⑤ 更改样本来源或类型，如来自不同网站的语料或者不同类型的语料等，重新计算上面②③④的结果, 进行对比分析。 

⑥ 完成一份技术报告，在报告中写明利用什么爬虫工具从哪些网站上收集的样本，如何进行的样本清洗，清洗后样本的规模，计算结果分析等。实验分析有较大的伸缩空间。



(C). 利用北京大学标注的《人民日报》1998年1月份的分词和词性标注语料（见SEP平台本课程附录03）完成如下工作： 

① 设计并实现算法，计算任意两个汉字之间的点式互信息和它们邻近同现的概率，分析两个邻近汉字构成词汇的概率与它们之间点式互信息情况。 

② 根据词性分类标记，计算任意两类词汇之间的互信息。 

③ 利用网络爬虫从《人民日报》官网(https ://paper.people.com.cn/rmrb/pc/layout/202510/15/node_01.html) 上爬取尽量多的语料（包括今年1月份的内容），对其进行处理，如清洗乱码等。 

④ 利用③中获取的今年1月份的语料与1998年1月的《人民日报》语料进行对比分析，分析词频差异，并抽取出所有的新词。 

⑤ 完成一份技术报告，在报告中写明上述各任务的计算结果分析情况，利用什么网络爬虫工具，如何进行的样本清洗等。结果分析有较大的伸缩空间。

**鉴于笔者在题目要求修改之前已经完成了作业，所以笔者选择了和原来相似度较大的问题一进行作业。**

### 具体内容见hw1中的README.md

### 总结

从上述数据可以看到，中英文语料的熵值和高频符号分布都相对稳定：中文语料在 2MB/5MB/10MB 三个快照中的熵值维持在 9.5~9.64 bits 之间，高频字始终集中在“的、国、中、一”等功能词或常用名词；英文语料的字母熵值则稳定在 4.16 bits 左右，高频字母分布符合英语普通文本 `ETAOIN` 的典型规律。在英文单词统计中，将所有撇号形式的所有格结尾（如 `China's`、`people’s`）视为整词处理后，孤立的 `s` 不再计入词频，因此 top-10 高频词更贴近常见功能词分布，整体熵值也随语料规模上升略有增加（约 10.29→10.43 bits），表明词级别的多样性与语料扩展保持正相关。总体而言随着语料规模扩大，字符覆盖度和去重数量逐渐提升，但整体熵值变化不大，说明单字符层面的统计特征在较小规模就已趋于收敛。后续分析可进一步考察不同时间段、题材或版面的差异，并尝试扩展到二元或 n 元模型，以捕捉更丰富的语言结构信息。


## Homework2（对应文件夹hw2）

### 题目

1. (简答题)

  请下载调试FNN、RNN和LSTM模型的开源工具。利用北京大学标注的《人民日报》1998年1月份的分词语料，或者利用网络爬虫自己从互联网上收集足够多的英文文本语料，借助FNN或者RNN/LSTM开源工具，完成如下任务，并撰写一份实验报告：

  (1) 获得汉语或英语词语的词向量. 

  (2) 对于同一批词汇，对比分别用FNN,RNN或LSTM获得的词向量的差异。 

  (3) 利用你认为最好的词向量结果，对于随机选取的20个词汇分别计算与其词向量最相似的前10个单词，按相似度大小排序，人工对比排序结果是否与你的判断一致。

  (4) [选做]如果汉语和英语的词向量都学习到了，请对比同一个意思的汉语词汇和英语词汇，如“书”和‘book’，“工作”和‘work/ job’ 等，分析其向量距离。



说明

- 如果计算资源的限制，神经网络参数不必选择过大，例如：词表选择1000个左右单词即可, 其余单词用代替；词向量的维度可设为10左右；神经网络的层数设置为1到2层； 

- 可以使用某一种开放的深度学习框架，如TensorFlow或者PyTorch。

- 如果不借助开源工具和开放的深度学习框架，题目中的某些任务可以不做。

### 具体内容见hw2中的README.md

### 总结
本次实验基于 PyTorch 实现了 FNN、RNN 和 LSTM 三种神经语言模型，并在中文（人民日报语料）和英文（WikiText-2）数据集上完成了训练与词向量提取。实验结果表明：
1. **模型性能**：LSTM 在捕捉长距离依赖方面表现最优，其生成的词向量在语义相似度任务中质量最高；RNN 次之，FNN 相对较弱。
2. **词向量特性**：通过余弦相似度分析，LSTM 训练出的词向量能较好地聚类语义相近的词汇（如“中国”与“美国”、“发展”与“建设”）。
3. **跨语言分析**：在选做部分，通过对比中英文对应词汇（如“书” vs “book”）的向量距离，发现虽然不同语言空间的向量分布不同，但在各自空间内的相对拓扑结构具有一定的同构性。


## Homework3（对应文件夹hw3）

### 题目
1. (简答题)

利用大语言模型（如 Qwen, Llama, ChatGLM 等）完成中文分词任务。

(1) **基准测试**：利用北京大学标注的《人民日报》1998年1月份的分词语料作为测试集，评估大语言模型在 Zero-shot 设定下的分词性能（Precision, Recall, F1）。

(2) **数据爬取与领域适应**：利用网络爬虫工具从互联网上收集不同领域的中文文本（如新闻、社交媒体、科普文章等），清洗后作为测试语料。评估大语言模型在这些非标准领域数据上的分词表现，并与传统分词工具（如 Jieba）进行对比。

(3) **模型规模对比**：对比同一系列不同参数规模的模型（例如 7B vs 14B vs 72B）在分词任务上的性能差异。

(4) **性能优化**：尝试使用 Prompt Engineering（如 Few-shot Prompting）或其他方法提升大语言模型的分词效果。

(5) **撰写报告**：完成一份实验报告，详细记录实验设置、爬虫策略、评估结果及分析结论。

### 具体内容见hw3中的README.md

### 总结
本项目探索了利用 Qwen-2.5 系列大语言模型进行中文分词的能力。
1. **基准性能**：LLM 在标准语料上 Zero-shot 表现良好，F1 值通常在 85%-90% 之间，展现了强大的语言理解能力。
2. **领域适应**：通过爬取新闻、社交媒体（豆瓣）、科普（果壳）三类语料进行评估，发现 LLM 在规范文本上表现最优，在口语化和专有名词较多的领域虽然略有下降，但相比传统规则/统计模型具有更好的泛化性。
3. **模型规模**：对比 7B、14B 和 72B 模型，发现参数量越大，模型对歧义词、长难句的处理越精准，F1 值随规模增加而提升。
4. **优化策略**：Few-shot Prompting 能够显著规范模型的输出格式，并提升对特定领域词汇的识别能力，是提升 LLM 分词性能的有效手段。



