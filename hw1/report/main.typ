#import "template.typ": *
#import "@preview/cram-snap:0.2.2": *
#set outline(title: "目录")

#show: bubble.with(
  title: "NLP第一次作业技术报告",
  author: "尹超",
  affiliation: "中国科学院大学\nUniversity of Chinese Academy of Sciences",
  // date: datetime.today().display(),
  subtitle: "关于中英文文本的爬取与概率和熵值计算",
  date: "2025.10.17",
  year: "2025",
  class: "2313AI",
  other: ("Template:https://github.com/hzkonor/bubble-template",),
  //main-color: "4DA6FF", //set the main color
  logo: image("../../cas_logo/CAS_logo.png"), //set the logo
  //"My GitHub:https://github.com/CarterYin"
) 

#outline()
#pagebreak()

// Edit this content to your liking

= Chapter 1 Introduction
本次作业内容如下：

1.分别收集尽量多的英语和汉语文本，编写程序计算这些文本中英语字母和汉字的熵，对比本章课件第18页上表中给出的结果。然后逐步扩大文本规模，如每次增加固定的数量，如2M/5M等，重新计算文本规模扩大之的熵，分析多次增加之后熵的变化情况。

要求：

- 利用爬虫工具从互联网上收集样本，并对样本进行处理，如清洗乱码等；
- 设计算法并编程实现在收集样本上字母/汉字的概率和熵的计算；
- 当改变样本规模时重新计算字母/汉字的概率和熵, 并对比计算结果；
- 完成一份技术报告，在报告中写明利用什么爬虫工具从哪些网站上收集的样本，如何进行的样本清洗，清洗后样本的规模，在不同样本规模下计算的结果等。实验分析有较大的伸缩空间。

= Chapter 2 Data Collection and Cleaning
== 爬虫类型和语料来源网站
本次实验中，我采取了定向（垂直）爬虫（Focused Web Crawler），从人民日报和China Daily两个网站上分别爬取了中文和英文文本数据。

== 爬虫实现流程与细节
我编写了两个独立的爬虫脚本，分别命名为 rm.py（针对人民日报）和 cd.py（针对 China Daily）。

=== 爬虫实现流程
两份爬虫脚本都是面向具体电子报网站的定向采集工具，整体流程相近：通过日期驱动生成索引页 URL，逐级解析出文章列表，再抓取正文并累积到单一语料文件；在此基础上又引入 CorpusMonitor，按 2MB/5MB/10MB 阈值自动截取快照，便于规模控制和阶段分析。rm.py 依赖较固定的人民网上版面结构，层层定位到文章正文；cd.py 则针对 China Daily 的 HTML 差异做了更复杂的选择器和清洗逻辑（如多套候选节点、元信息剔除、重复段落去除），以适应版式变化。两者都使用 requests + BeautifulSoup 组合处理静态页面，并在请求层面设置通用的 UA/超时、在抓取循环中加延时，以降低被封风险，属于典型的模板化垂直爬虫实现。

=== 爬虫实现细节
细节处理方面，爬虫脚本均实现了断点续爬功能，利用本地文件记录已抓取日期，避免重复下载。文本清洗主要包括 HTML 标签剥离、空白字符归一化、非正文内容过滤、乱码去除等，确保语料纯净度。最终的语料文件均采用 UTF-8 编码保存，便于后续处理。

== 爬取结果
爬取结果方面，人民日报从20241201爬取到20250301，China Daily 则从20241201爬取到20250401。分别成功获得了三份不同大小的语料文件，大小为 2MB/5MB/10MB，均为纯文本格式，编码为 UTF-8。检查显示，文本内容较为干净，无乱码，符合后续处理要求。

= Chapter 3 Probability and Entropy Calculation
在获得清洗后的文本语料后，下一步是计算字母/汉字的概率分布及熵值。具体步骤如下：

== 中文字符的概率与熵值计算
#strong[符号筛选与计数]：针对中文语料，使用 `compute_rmrb.py` 中的 `iter_cjk_chars` 函数过滤 CJK 统一汉字范围，再用 `collections.Counter` 统计字符频次；针对英文语料，`compute_cd.py` 采用 `iter_english_letters` 过滤 ASCII 字母并按需小写化。

#strong[概率与熵计算]：对每个字符取频次占比得到概率分布 `p_i`，随后使用香农熵公式 `H = -∑ p_i log₂ p_i` 计算信息熵，并记录字符总数、去重后字符数等指标。

#strong[结果展示]：脚本支持通过 `--top` 参数输出高频符号明细，包含排名、字符、频次、概率、信息量（bits）与贡献度，便于直观对比。

=== 以 `data/rmrb_snapshot_2MB.txt` 为例

运行 `python compute_rmrb.py data\rmrb_snapshot_2MB.txt --top 10` 得到如下输出：

```bash
(nlp) PS E:\homework\nlp\hw1> python compute_entropy.py data\rmrb_snapshot_2MB.txt --top 10
Corpus: data\rmrb_snapshot_2MB.txt
Total Chinese characters: 602680
Unique Chinese characters: 3571
Shannon entropy: 9.533166 bits

Top characters:
rank char     count    prob      information(bits)  contribution
   1  的      12996  0.021564        5.535253         0.119360
   2  国       7551  0.012529        6.318581         0.079166
   3  中       6743  0.011188        6.481858         0.072521
   4  一       5608  0.009305        6.747762         0.062789
   5  发       4736  0.007858        6.991579         0.054941
   6  新       4110  0.006820        7.196110         0.049074
   7  化       4006  0.006647        7.233086         0.048078
   8  人       3977  0.006599        7.243568         0.047799
   9  和       3966  0.006581        7.247564         0.047693
  10  业       3916  0.006498        7.265868         0.047211
```

可以看到高频汉字集中在“的、国、中、一”等常用词，单个字符的信息量约 5.5～7.3 bits，整体熵值 9.53 bits 与课堂参考数据基本一致。

=== 当语料扩展到 5MB 与 10MB 快照时

```bash
(nlp) PS E:\homework\nlp\hw1> python compute_entropy.py data\rmrb_snapshot_5MB.txt --top 10
Corpus: data\rmrb_snapshot_5MB.txt
Total Chinese characters: 1503549
Unique Chinese characters: 4307
Shannon entropy: 9.639168 bits

Top characters:
rank char     count    prob      information(bits)  contribution
   1  的      33439  0.022240        5.490696         0.122113
   2  国      16802  0.011175        6.483595         0.072453
   3  中      15766  0.010486        6.575411         0.068949
   4  一      13521  0.008993        6.797026         0.061124
   5  发      11182  0.007437        7.071050         0.052588
   6  人      10897  0.007248        7.108297         0.051518
   7  业       9574  0.006368        7.295034         0.046452
   8  和       9550  0.006352        7.298655         0.046358
   9  在       9487  0.006310        7.308204         0.046113
  10  年       9333  0.006207        7.331815         0.045511
```

```bash
(nlp) PS E:\homework\nlp\hw1> python compute_entropy.py data\rmrb_snapshot_10MB.txt --top 10
Corpus: data\rmrb_snapshot_10MB.txt
Total Chinese characters: 3006557
Unique Chinese characters: 4735
Shannon entropy: 9.631801 bits

Top characters:
rank char     count    prob      information(bits)  contribution
   1  的      66909  0.022254        5.489768         0.122171
   2  国      32913  0.010947        6.513311         0.071302
   3  中      30465  0.010133        6.624816         0.067128
   4  一      26290  0.008744        6.837454         0.059788
   5  人      22374  0.007442        7.070145         0.052614
   6  发      21741  0.007231        7.111550         0.051425
   7  业      20417  0.006791        7.202198         0.048909
   8  和      19202  0.006387        7.290712         0.046564
   9  在      18814  0.006258        7.320162         0.045807
  10  年      18745  0.006235        7.325463         0.045672
```


`compute_rmrb.py` 输出的熵值分别为 9.639 bits 与 9.632 bits，去重字符数增至 4307 与 4735，但高频字仍围绕“的、国、中、一、人、发”等词汇，所占概率与贡献度变化不大。这说明在人民日报的报道语域中，单字符层面的分布在几兆规模下迅速收敛，捕捉到的多为功能词与核心名词。

== 英文字符的概率与熵值计算
=== 纵观所有结果
```bash
(nlp) PS E:\homework\nlp\hw1> python compute_cd.py data\cd_snapshot_2MB.txt --top 10
Corpus: data\cd_snapshot_2MB.txt
Total letters: 1706620
Unique letters: 26
Shannon entropy: 4.162821 bits

Top letters:
rank char    count    prob       information(bits) contribution
   1  e     200370  0.117408        3.090403         0.362837
   2  t     149595  0.087656        3.512008         0.307848
   3  i     144565  0.084708        3.561352         0.301676
   4  a     143569  0.084125        3.571326         0.300437
   5  n     141317  0.082805        3.594135         0.297613
   6  o     125017  0.073254        3.770946         0.276237
   7  s     111119  0.065111        3.940964         0.256598
   8  r     107151  0.062786        3.993425         0.250729
   9  h      73296  0.042948        4.541264         0.195038
  10  l      68587  0.040189        4.637063         0.186358
```

```bash
(nlp) PS E:\homework\nlp\hw1> python compute_cd.py data\cd_snapshot_5MB.txt --top 10
Corpus: data\cd_snapshot_5MB.txt
Total letters: 4264958
Unique letters: 26
Shannon entropy: 4.162419 bits

Top letters:
rank char    count    prob      information(bits)  contribution
   1  e     500480  0.117347        3.091147         0.362737
   2  t     373307  0.087529        3.514097         0.307585
   3  i     361563  0.084775        3.560213         0.301818
   4  a     360869  0.084613        3.562984         0.301473
   5  n     352718  0.082701        3.595944         0.297390
   6  o     311461  0.073028        3.775408         0.275710
   7  s     279295  0.065486        3.932670         0.257535
   8  r     268068  0.062854        3.991861         0.250903
   9  h     182403  0.042768        4.547330         0.194479
  10  l     170040  0.039869        4.648585         0.185335
```

```bash
(nlp) PS E:\homework\nlp\hw1> python compute_cd.py data\cd_snapshot_10MB.txt --top 10
Corpus: data\cd_snapshot_10MB.txt
Total letters: 8532582
Unique letters: 26
Shannon entropy: 4.162832 bits

Top letters:
rank char count prob information(bits) contribution
   1  e    1000965  0.117311        3.091591         0.362677
   2  t     746638  0.087504        3.514502         0.307534
   3  i     723501  0.084793        3.559915         0.301855
   4  a     721101  0.084511        3.564709         0.301259
   5  n     704407  0.082555        3.598501         0.297074
   6  o     619315  0.072582        3.784237         0.274669
   7  s     562327  0.065903        3.923501         0.258572
   8  r     537203  0.062959        3.989443         0.251171
   9  h     362041  0.042430        4.558757         0.193430
  10  l     341302  0.040000        4.643862         0.185754
```

=== 结果分析

对于英文语料，`compute_cd.py` 在不同快照下统计到的 26 个字母概率基本一致，熵值始终落在 4.162~4.163 bits 区间。由于脚本默认将大小写折叠为小写字符，分布主要反映了标准英文的基础字母频率，高频顺序呈现出 `e > t > i/a > n > o` 的经典“ETAOIN”形态。

在 2MB 样本中，总字母数约 170 万，`e` 单字母的概率达到 0.117，单独贡献 0.36 bits；中段的 `o`、`s`、`r` 探测到的概率持续落在 0.06~0.07 范围内，对整体熵的贡献维持在 0.25~0.28 bits；而尾部如 `h`、`l` 的概率则在 0.04 左右，信息量上升到 4.5 bits 以上，但贡献度显著低于主频字母。随着规模扩大到 5MB 与 10MB，总字母数依次增至 4.26M 与 8.53M，各字母的概率、信息量与贡献度几乎未发生肉眼可察的变化，说明英语单字母分布在百万级样本中已经充分体现，后续数据更多是在降低统计噪声。

与中文语料相比，英文熵值更低（约 4.16 bits vs. 9.6 bits），这反映出 26 个字母构成的符号集更紧凑、平均概率差异更大；而中文汉字数量庞大且长尾效应明显，使得熵值更高。两种语料在扩大规模后都显示出稳定的单字符分布，但中文在扩展快照时去重字符仍持续增长，表明长期仍有大量低频汉字被逐步纳入，而英文字母集合在初始阶段就已完整覆盖。

综上，英语语料的单字符熵稳定性与经典语言学结论高度一致，也验证了脚本的处理逻辑。若想进一步探究版面、主题或时间段的差异，需要引入双字母或更高阶的 n 元模型，或结合词级统计与停用词过滤，才能捕捉到语义层面对分布的微妙变化。

= Chapter 4 Conclusion
通过本次实验，我构建了从数据采集到统计分析的完整流程：先针对人民日报与 China Daily 制作定向爬虫，结合 `CorpusMonitor` 在关键体积截取快照，再以 `compute_rmrb.py` 与 `compute_cd.py` 对语料进行单字符层面的概率与熵值计算。整个实践强化了“高质量数据 + 精确统计”这一基本功：爬虫阶段的结构化解析、正文抽取与编码清洗直接决定了后续统计的可信度，而脚本化的熵计算则提供了量化语言冗余度与信息密度的手段。

实验结果显示，中文汉字的熵值稳定在约 9.6 bits，英文字母约 4.16 bits，两者在 2MB/5MB/10MB 的规模扩展后仍保持高度一致的高频符号分布。这一现象印证了语言统计的“早期收敛”特性：即便语料继续累积，单字符概率的主干结构不会发生突变，新增数据更多是在补充长尾符号并降低采样噪声。另一方面，中文熵值显著高于英文，既反映了汉字数量的巨大与语素表达的丰富，也提醒我们在中文 NLP 中需要面对更大的符号空间与稀疏挑战。

本次作业还暴露了一些值得深入的方向。例如，快照机制固然方便观察总体趋势，但若要比较不同板块（如财经、文化）或不同时间段的用字差异，还需在爬虫阶段记录更细粒度的元数据，并在统计脚本中支持分组或增量分析。此外，单字符熵只能捕捉零阶语言特征，要理解句法与语义层面，必须引入二元、三元乃至子词/词级模型，配合停用词处理和语言模型评估，才能揭示更深层的语言组织规律。

综上所述，这次实践不仅完成了作业要求，也为后续的深入研究奠定了基础：一方面掌握了构建垂直爬虫与清洗语料的工程方法，另一方面通过熵值分析认识到语言统计的稳定性与差异性。未来可以在此框架上继续扩展，如引入自动化监控、可视化仪表板或与云端数据管道结合，以支持更大规模、更细粒度的文本分析任务。


